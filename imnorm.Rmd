---
output: 
  html_fragment:
    keep_md: true
---



```{r echo = FALSE}
library(knitr)
opts_chunk$set(echo = TRUE, cache = TRUE)
```

Use statistics and R instead of squinting at satellite images!

<!-- more -->
You may have, like me, run into this [article](http://bits.blogs.nytimes.com/2014/08/13/start-up-provides-a-picture-of-our-shape-shifting-planet/). Amazing stuff. A little startup pushing satellite imaging to the next level. Full planet coverage at the resolution of a few feet every 24 hours, soon, and on a shoestring budget. The article is rich in images. Let's load them for later use (I an sure this is fair use, bit if folks at Planet labs are upset by this, I will oblige). I used a low tech right click to get them to my local drive. No screen scraping in this episode, sorry.

```{r cache=FALSE, echo=FALSE}
library(jpeg)
library(grid)
library(abind)
library(ggplot2)
options(warn = -1)
```
```{r}
fnames = 
  list(
    ulanqab = "~/Downloads/planet-labs-1.jpg",
    lake.county = "~/Downloads/planet-labs-2.jpg",
    tres.marias = "~/Downloads/planet-labs-3.jpg",
    han.river = "~/Downloads/planet-labs-4.jpg")
```

As usual we need some data laundry. Two images of the same area at different moments in time are pasted vertically into one larger image. Not a big deal.

```{r}
split.image  =
  function(image)
    list(
      before = image[1:360,,],
      after = image[369:728,,])

images = lapply(fnames, function(fn) split.image(readJPEG(fn)))
```


Now we have the images in convenient arrays, we need to plot them. Here are a few handy functions

```{r}
myplot = function(...) UseMethod("myplot")

myplot.matrix = 
  function(red, green, blue) {
    im = rgb(red, green, blue)
    dim(im) = dim(red)
    grid.raster(im)}

myplot.array = 
  function(arr)
    myplot(arr[,,1], arr[,,2], arr[,,3])
```    

What this says is that images are stored in  arrays with three dimensions, x, y and channel (red, green and blue). Using the library `grid`, we create an `rgb` object and plot it. The functions are organized as one generic with two methods, one for matrices (single channel) and one for arrays (images). Let's take a look.

Past:

```{r}
with(images$ulanqab, myplot(before))
```

Present:

```{r}
with(images$ulanqab, myplot(after))
```

I don't know about you, but the only thing I can see right away is that one image is a lot warmer and a little brighter than the other. It could be that there was a dry season followed by a rainy season, or it could be that the equipment used for the two images is different and results in a different color temperature. In fact the older picture is by USGS/NASA and the newer one by Planet Labs, so no wonder there are differences in equipment. A measured difference that does not depend on a change in the phenomenon of interest is called artifact or confounding factor and there is often a need to remove it or discount it in data analysis. This operation is often called *normalization*. There is a risk that normalization could remove real differences, not unlike when we use flash photography at night, it ruins the atmosphere, doesn't it? But we should have calibrated our instruments early on; it's too late now to complain. These differences are suspect, so we want them out of our images.


 But how to go about it? In this case, it may be sufficient to apply a linear transformation to each channel, that is adjust brightness and contrast for each color, seen as a monochrome image, until the pictures have the same overall brightness and contrast. But let's try something more interesting or, should I say, more assumption-free. In my days in biotech we used a normalization technique called [*quantile normalization*](https://en.wikipedia.org/wiki/Quantile_normalization). The images we were dealing with were affected by non-linear effects that a linear correction could not fix. Imagine the low lights and highlights are fine and the midtones are too dark. Quantile normalization deals with that. It applies a monotone transformation to the pixels of one image, so that the distribution of intensities becomes the same as in a reference image. It's a nonparametric method, so it's not bound to a particular functional form of this correction, linear or otherwise. There is one problem with this technique. It assumes that the distribution of intensities should be the same in the ideal images. This is not always true. Imagine you are taking pictures of a soccer match. You are shooting from a high position, so that in the frame there are always 22 people and a ball. They move around, but the overall distribution of intensities in different pictures is the same, so much green grass, some red and blue jerseys and the ball is white and black. If you underexpose a picture by mistake, not to worry, just apply quantile normalization and you get a usable shot again. Now you move close to the field. Sometimes the players are far away and sometimes just in front of you. The mixture of intensities in your pictures keeps changing: more green, less green, more blue, less blue. You shoot an underexposed picture by mistake and want to apply quantile normalization to fix it. Unfortunately, you don't know which picture to use as a reference, because they all have different distributions of intensities. If you, say, grab a picture with less grass in it than the picture to be corrected, quantile normalization will remove grass from the picture to be corrected and sprinkle some jerseys instead. The assumptions don't hold and chaos ensues. 
 
So here is a new idea, at least after a quick literature search it appears to be new. If somebody knows it already, please let me know and I will add appropriate references. The goal is to define a non-linear correction of the intensities that can only reduce the difference between two images, and never add new ones. I will call this "conservative differential normalization". I wish I had it in my biotech days, as in biology differences are the only thing that counts. We just start with a scatter plot of the intensities in one image against the intensities in the other -- all the steps here are applied one channel at a time -- and a smoothing line for good measure. 

```{r echo =TRUE, message=FALSE}
ula = 
  data.frame(
    x = as.vector(images$ulanqab$before[,,2]), 
    y = as.vector(images$ulanqab$after[,,2]))
ggplot(data = ula, aes(x = x, y = y)) + geom_jitter(size = .7) +  geom_smooth()
```

Now we use the smoother as a "translator" from intensities in one image to those in the other, which means we have an intensity dependent normalization function, that, due to the robustness of the smoother, doesn't respond to rare, outlying changes, which we want to preserve. In our soccer analogy, the smoother will be guided by the fact that most grass is grass in both images and map green to green, and refuse to  replace it with red or blue. If all the grass is darker in one image than the other, the smoother will be away from the diagonal in the range of intensities corresponding to grass, and make it look the same in both images. In the following implementation, one added touch is that the reference image for two images that need to be normalized is always the average image. I don't think that's technically necessary, but visually it is more appealing to see images converge to a "consensus" and it's also easier to generalize to many images, where the median may be preferred for its robustness.


```{r}
smunorm = 
  function(...) UseMethod("smunorm")

smunorm.matrix = 
  function(im1, im2) {
    imavg = (im1 + im2)/2
    smoother = do.call(approxfun, supsmu(im1, imavg))
    im1 = apply(im1, 2, smoother)
    ifelse(
      im1 < 0,
      0,
      ifelse(
        im1 > 1,
        1,
        im1))}
    
smunorm.array = 
  function(arr1, arr2) 
      abind(
        smunorm(arr1[,,1], arr2[,,1]),
        smunorm(arr1[,,2], arr2[,,2]),
        smunorm(arr1[,,3], arr2[,,3]),
        along = 3)
```

In detail here we are using the `supsmu` function at defaults and are performing linear interpolation with `approxfun`.  Finally normalized intensities are truncated to the $[0,1]$ interval. I am not sure why they exceed it in the first place, but it happens only occasionally and by modest amounts, so I wouldn't worry about it. And these are the normalized images:

```{r}
with(images$ulanqab, myplot(smunorm(before, after)))
```
```{r}
with(images$ulanqab, myplot(smunorm(after, before)))
```

Great! Now the pictures are more similar and we can start to focus on the details: I see that the irrigation status of some of those round patches has changed, and it looks like a grassy field is now a pond. But my eyesight is not what it used to be and I get tired of squinting at these pictures, and there are many of them. ["Can't someone else do it?"](https://en.wikipedia.org/wiki/Trash_of_the_Titans), for instance, our trusty computer? We could take the difference of images and try to make it into an image again, but what defines an exceptional difference, one worth looking into, could change from picture to picture. We need something more invariant from image pair to another, like a probabilistic model of differences between two normalized images. After eyeballing a few `qqnorm` plots for up to 10 minutes, I decided I was going to consider the differences between images as a mixture of a normal distribution and something else corresponding to small uninteresting differences and big interesting differences respectively, and rate them based on the probability of being generated by the normal component. The parameters of the normal are estimated robustly with `median` and `mad`. Should do it for a blog post, but don't bet the company on this method just yet.

```{r}
imdiff = function(...) UseMethod("imdiff")

imdiff.matrix = 
  function(im1, im2) {
    imd = im1 - im2
    sd = mad(imd)
    mean = median(imd)
    imd = dnorm(imd, mean = mean, sd = sd)
    r = range(imd)
    ((imd) - r[1]) / diff(r)}

imdiff.array = 
  function(im1, im2)
    abind(
      imdiff(im1[,,1], im2[,,1]),
      imdiff(im1[,,2], im2[,,2]),
      imdiff(im1[,,3], im2[,,3]), 
      along = 3)
```

And here it is in action; darker means large difference and saturated means channel-specific difference.

```{r}
with(images$ulanqab, myplot(imdiff(smunorm(after, before), smunorm(before, after))))
```

Not only the changes in the farmland are more clear, with some plots going from dry to verdant and the other way around, but also the cluster of changes around the urban area in the top left corner is more visible. Also, try to locate the meadow that becomes a reservoir (hint: look for a purple patch).

There were a few more pictures in that article, why don't we take a look at what happens?

The crossing of the Han river. Past:

```{r}
with(images$han.river, myplot(before))
```

Present:

```{r}
with(images$han.river, myplot(after))
```

Normalized:

```{r}
with(images$han.river, myplot(smunorm(before, after)))
```

```{r}
with(images$han.river, myplot(smunorm(after, before)))
```

Difference plot:

```{r}
with(images$han.river, myplot(imdiff(smunorm(after, before), smunorm(before, after))))
```

Interesting how the man made structures stand out, and the change in water clarity.

The next example shows the effects of drought on a reservoir in Brasil. Past:
```{r}
with(images$tres.marias, myplot(before))
```

Present:

```{r}
with(images$tres.marias, myplot(after))
```

Normalized:

```{r}
with(images$tres.marias, myplot(smunorm(before, after)))
```

```{r}
with(images$tres.marias, myplot(smunorm(after, before)))
```

Difference plot:

```{r}
with(images$tres.marias, myplot(imdiff(smunorm(after, before), smunorm(before, after))))
```

The main changes here are around the shoreline, the color of the water at the southern end of the reservoir and some vegetation changes on the ridges surrounding the reservoir, in turquoise.

Effects of drought on a reservoir in California:

Past:

```{r}
with(images$lake.county, myplot(before))
```

Present:

```{r}
with(images$lake.county, myplot(after))
```

Normalized:

```{r}
with(images$lake.county, myplot(smunorm(before, after)))
```

```{r}
with(images$lake.county, myplot(smunorm(after, before)))
```

Difference plot:

```{r}
with(images$lake.county, myplot(imdiff(smunorm(after, before), smunorm(before, after))))
```

Here again the changes are present all along the shoreline, but also the color of most of the water surface has turned darker.

## Conclusion

Don't squint at satellite images! Let computers do the hard work and use R to program them.

### Materials and Methods
Images by [Planet Labs](planet.com) and [USGS](usgs.gov)/[NASA](nasa.gov). Helpful packages: `jpeg` (read jpegs), `grid` (raster plot), `abind` (array manipulations), `ggplot2` (statistical graphics), `knitr` (report generation) . Written in R Markdown with [Rstudio](rstudio.com).

### Copyright and license

Copyright 2014 Antonio Piccolboni. The code in this post is licensed under the Apache 2.0 license http://www.apache.org/licenses/LICENSE-2.0 and is distributed "as is", without warranties of any kind.  The text in this post is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.