---
title: "Can't someone else find those differences?"
output: html_document
---


```{r echo = FALSE}
library(knitr)
opts_chunk$set(echo = FALSE, cache = TRUE)
options(rstudio.markdownToHTML = 
            function(inputFile, outputFile) {      
                require(markdown)
                markdownToHTML(inputFile, outputFile, fragment.only=T)
            }
)
```


You may have, like me, run into this [article](http://bits.blogs.nytimes.com/2014/08/13/start-up-provides-a-picture-of-our-shape-shifting-planet/). Amazing stuff. A little startup pushing satellite imaging to the next level. Full planet coverage at the resolution of a few feet every 24 horus, soon, and on a shoestring budget. The article is rich in images. Let's load them for later use (I an sure this is fair use, bit if folks at Planet labs are upset by this, I will oblige). I used a low tech right click to get them to my local drive. No screen scraping in this episode, sorry.

```{r cache=FALSE, echo=FALSE}
library(jpeg)
library(grid)
library(abind)
library(ggplot2)
options(warn = -1)
```
```{r}
fnames = 
  list(
    ulanqab = "~/Downloads/planet-labs-1.jpg",
    lake.county = "~/Downloads/planet-labs-2.jpg",
    tres.marias = "~/Downloads/planet-labs-3.jpg",
    han.river = "~/Downloads/planet-labs-4.jpg")
```

As usual we need some data laundry. Two images of the same area at different moments in time are pasted vertically into one larger image. Not a big deal.

```{r}
split.image  =
  function(image)
    list(
      before = image[1:360,,],
      after = image[369:728,,])

images = lapply(fnames, function(fn) split.image(readJPEG(fn)))
```


Now we have the images in convenient arrays, we need to plot them. Here are a few handy functions

```{r}
myplot = function(...) UseMethod("myplot")

myplot.matrix = 
  function(red, green, blue) {
    im = rgb(red, green, blue)
    dim(im) = dim(red)
    grid.raster(im)}

myplot.array = 
  function(arr)
    myplot(arr[,,1], arr[,,2], arr[,,3])
```    

What this says is that images are stored in  arrays with three dimensions, x, y and channel (red, green and blue). Using the library grid, we create and rgb object and plot it. Easy stuff. Let's take a look.

Past:

```{r}
with(images$ulanqab, myplot(before))
```

Present:

```{r}
with(images$ulanqab, myplot(after))
```

I don't know about you, but the only thing I can see is that one image is a lot warmer and a little brighter than the other. It could be that there was a dry season followed by a rainy season, or it could be that the equipment used for the two images is different and results in a different color temperature. In fact the older picture is by USGS/NASA and the newer one by Planetlabs, so no wonder there are differences in equipment. A measured difference that does not depend on a change in the phenomenon of interest is called artifact or confounding factor and there is often a need to remove it or discount it in data analysis. This operation is often called *normalization*. But how to go about it? In this case, it may be sufficient to apply a well calibrated linear transformation to each channel, that is adjust brightness and contrast for each color, seen as a monochrome image, untile the pictures have the same overall brightness and contrast. Of course there is a possibility that we are also removing real differences, such as when we use flash photography at night, it ruins the atmosphere, doesn't it? But we should have calibrated our instruments early on; it's too late now to complain. These differences are suspect, so we want them out of our images. But let's try something more fun than a linear transformation or more assumption-free. In my days in biotech we used a normalization technique called *quantile normalization*. The images we were dealing with were affected by non-linear effects that a linear correction of contrast an brightness could not fix. Imagine the low lights and highlights are fine and the midtones are way too dark. How do you linearly correct that? Impossible. Quantile normalization deals with that. It applies a monotone transformation to the pixels of one image, so that the distribution of intensities becames the same as in a reference image. It's a nonparametric method, so it's not bound to a particular functional form of this correction, linear or otherwise. There is one problem with this technique. It assumes that the distribution of intensities should be the same in the ideal images. This is not always true. Imagine you are taking pictures of a soccer match. You are shooting from a high position, so that in the frame there are always 22 people and a ball. They move around, but the overall distribution of intensities in different pictures is the same, so much green from the grass, some red and blue from the jerseys and the ball is white and black. If you underexpose a picture by mistake, not to worry, just apply quantile normalization and you get a usable shot again. Now you move close to the field. Sometimes the players are far away and sometimes just in front of you. The mixture of intensities in your pictures keeps changing: more green, less green, more blue, less blue. You shoot an underexposed picture by mistake and want to apply quantile normalization to fix it. Unfortunately, you don't know which picture to use as a reference, because they all have different distributionsof intensities. If you, say, grab a picture with less grass in it than the picture to be corrected, quantile normalization will remove grass from the picture to be corrected and plant some jerseys instead. The assumptions don't hold and chaos ensues (luckily, or maybe not, this technique is used for gene expression analysis, where errors such as this are much harder to detect, particularly for reviewers). So here is a new idea, at least after a quick literature search it appears to be new. If somebody knows it already, please let me know and I will add appropriate references. The goal is to define a non-linear correction of the intensities that can only reduce the difference between two images, and never add new ones. I will call this "conservative differential normalization". I wish I had it in my biotech days, as in biology differences are the only thing that counts. We just start with a scatter plot of the intensities in one image against the intensities in the other -- all the steps here are applied one channel at a time -- and a smoothing line for good measure. 

```{r echo =TRUE, message=FALSE}
ula = 
  data.frame(
    x = as.vector(images$ulanqab$before[,,2]), 
    y = as.vector(images$ulanqab$after[,,2]))
ggplot(data = ula, aes(x = x, y = y)) + geom_jitter(size = .7) +  geom_smooth()
```

Now we use the smoother as a *translation* from intensities in one image to those in the other, which means we have an intensity dependent normalization function, that, due to the robustness of the smoother, doesn't respond to rare, outlying changes, which we want to preserve. In our soccer analogy, the smoother will be guided by the fact that most grass is grass in both images and map green to green, and refuse to plant jerseys instead. If all the grass is darker in one image than the other, the smoother will be away from the diagonal in the range of intensities corresponding to grass, and make it look the same in both images. In the following implementation, one added touch is that the reference image for two images that need to be normalized is always the average image. I don't think that's technically necessary, but visually it is more appealing to see images converge to a "consensus" and it's also easier to generalize to many images, where the median may be preferred for its robustness.


```{r}
smunorm = 
  function(...) UseMethod("smunorm")

smunorm.matrix = 
  function(im1, im2) {
    imavg = (im1 + im2)/2
    smoother = do.call(approxfun, supsmu(im1, imavg))
    im1 = apply(im1, 2, smoother)
    ifelse(
      im1 < 0,
      0,
      ifelse(
        im1 > 1,
        1,
        im1))}
    
smunorm.array = 
  function(arr1, arr2) 
      abind(
        smunorm(arr1[,,1], arr2[,,1]),
        smunorm(arr1[,,2], arr2[,,2]),
        smunorm(arr1[,,3], arr2[,,3]),
        along = 3)
```

In detail here we are using the `supsmu` function at defaults and are performing linear interpolation with `approxfun`.  Finally normalized intensities are truncated to the $[0,1]$ interval. I am not sure why they exceeded it in the first place, but the exceptions were only slightly outside this interval, so I didn't worry about it. And these are the normalized images:

```{r}
with(images$ulanqab, myplot(smunorm(before, after)))
```
```{r}
with(images$ulanqab, myplot(smunorm(after, before)))
```

Great! Now the pictures are more similar and we can start to focus on the details: I see that the irrigation status of some of those round patches has changed, and it looks like a grassy field is now a pond. But my eyesight is not what it used to be and I get tired of squinting at these pictures, and there's many of them.  ["Can't someone else do it?"](https://en.wikipedia.org/wiki/Trash_of_the_Titans), for instance, our trusty computer? Of course we can take the difference of images and try to make it into an image again, but what defines an exceptional difference, on worth looking into, could change from picture to picture. We need something more invariant from image pair to another. So we need a probabilistic model of differences bewteen two normalized images. After eyeballing a few `qqnorm` plots for, like, 10 minutes, I  decided I was going to consider the differences between images as a mixture of a normal distribution and something else corresponding to small uninteresting differences and big interesting differences respectively, and rate them based on the probability of being generated by the normal component. The parameters of the normal are estmated robustly with `median` and `mad`. Should do it for a blog post, but don't bet the company on this method just yet.

```{r}
imdiff = function(...) UseMethod("imdiff")

imdiff.matrix = 
  function(im1, im2) {
    imd = im1 - im2
    sd = mad(imd)
    mean = median(imd)
    imd = dnorm(imd, mean = mean, sd = sd)
    r = range(imd)
    ((imd) - r[1]) / diff(r)}

imdiff.array = 
  function(im1, im2)
    abind(
      imdiff(im1[,,1], im2[,,1]),
      imdiff(im1[,,2], im2[,,2]),
      imdiff(im1[,,3], im2[,,3]), 
      along = 3)
```

And here it is in action; darker means large difference and saturated means channel-specific difference.

```{r}
with(images$ulanqab, myplot(imdiff(smunorm(after, before), smunorm(before, after))))
```

Not only the changes in the farmland are more clear, with some plots going from dry to verdant and viceversa, but also the cluster of changes around the urban area in the top left corner are more visible. Also, try to locate the medow that becomes a reservoir (hint, look for a purple patch).

There were a few more pictures in that article, why don't we take a look at what happens?

The crossing of the Han river. Past:

```{r}
with(images$han.river, myplot(before))
```

Present:

```{r}
with(images$han.river, myplot(after))
```

Normalized:

```{r}
with(images$han.river, myplot(smunorm(before, after)))
```

```{r}
with(images$han.river, myplot(smunorm(after, before)))
```

Difference plot:

```{r}
with(images$han.river, myplot(imdiff(smunorm(after, before), smunorm(before, after))))
```

Interesting how the man made structures stand out, and the change in water clarity.

The next example shows the effects of drought on a reservoir in Brasil. Past:
```{r}
with(images$tres.marias, myplot(before))
```

Present:

```{r}
with(images$tres.marias, myplot(after))
```

Normalized:

```{r}
with(images$tres.marias, myplot(smunorm(before, after)))
```

```{r}
with(images$tres.marias, myplot(smunorm(after, before)))
```

Difference plot:

```{r}
with(images$tres.marias, myplot(imdiff(smunorm(after, before), smunorm(before, after))))
```

The main changes here are around the shoreline, the shallowness of the southern end of the reservoir and some vegetation changes on the ridges surrounding the reservoir, in turquoise.

Effects of drought on a reservoir in California:

Past:

```{r}
with(images$lake.county, myplot(before))
```

Present:

```{r}
with(images$lake.county, myplot(after))
```

Normalized:

```{r}
with(images$lake.county, myplot(smunorm(before, after)))
```

```{r}
with(images$lake.county, myplot(smunorm(after, before)))
```

Difference plot:

```{r}
with(images$lake.county, myplot(imdiff(smunorm(after, before), smunorm(before, after))))
```

Conclusions

A little bit of R and a little bit of stats can help extract more information from the same data.


### Materials and Methods
Images by Planetlabs and USGS/NASA. Helpful packages: `jpeg` (read jpegs), `grid` (raster plot), `abind` (array manypulations), `ggplot2` (statistical graphics), `knitr` (report generation) . Written in RMarkdown with Rstudio.